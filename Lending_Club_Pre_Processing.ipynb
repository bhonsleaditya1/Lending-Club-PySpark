{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lending-Club-Pre-Processing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SHpMH_QR8YXu",
        "O6jAn98B6Lm0",
        "1vtYVuXJ8p7J",
        "JJKRn-z76YIL"
      ],
      "authorship_tag": "ABX9TyO0AZ3ay20WY6rK1IhcDK5L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhonsleaditya1/Lending-Club-PySpark/blob/master/Lending_Club_Pre_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXRhmtqS5Gxq"
      },
      "source": [
        "#Setting up Spark Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ049JQ046YC"
      },
      "source": [
        "##Installing PySpark & findspark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEi-Zx3I3wJe"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1G-TlL05D2Z"
      },
      "source": [
        "##Setting Path Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjv4xbs05DJw"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa8M9i455Sfg"
      },
      "source": [
        "##Building Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CKwaV7o5RGS"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LLALRD85bFQ"
      },
      "source": [
        "##Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTzMTig75aVf"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyspark.sql.functions as f\n",
        "from tqdm import tqdm\n",
        "from pyspark.sql.types import IntegerType,DateType,DoubleType,StringType\n",
        "from pyspark.ml.feature import QuantileDiscretizer\n",
        "from pyspark.sql.functions import log\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql import Window "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfkW_enN5sFc"
      },
      "source": [
        "#Data Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaBB11-w5lNu"
      },
      "source": [
        "##Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfJR3TNI5kJz"
      },
      "source": [
        "dataset = spark.read.csv('/content/drive/My Drive/Lending-Club/loan.csv',inferSchema=True, header =True)\n",
        "df = dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owar2kYf6DH2"
      },
      "source": [
        "##Exploring Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT4oYMjy6CT4"
      },
      "source": [
        "df.select(df.,df.loan_amnt).toPandas().groupby('loan_status').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr9BZvfnCldP"
      },
      "source": [
        "##Calculating Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgbMuUdBCzLL"
      },
      "source": [
        "from tqdm import tqdm \n",
        "stats = {}\n",
        "for col in tqdm(df.columns):\n",
        "  stats[col] = df.select(col).summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBfp9ctCCkTo"
      },
      "source": [
        "p = pd.DataFrame(index=None,columns=[])\n",
        "norows = df.count()\n",
        "for col in stats:\n",
        "  pdf = stats[col].toPandas()\n",
        "  pdf_T = pdf.T\n",
        "  pdf_T.columns = pdf_T.iloc[0]\n",
        "  pdf_T = pdf_T[1:]\n",
        "  p=p.append(pdf_T)\n",
        "p['fill%'] = (p['count'].astype(int)/norows)*100\n",
        "p.to_excel(\"Statistics.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIr2nnrx5wue"
      },
      "source": [
        "#Cleaning Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c960KK2x78F-"
      },
      "source": [
        "##Defining Schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaR6BLxW5qRC"
      },
      "source": [
        "schema = pd.read_csv('/content/drive/My Drive/Lending-Club/schema.csv').values.tolist()\n",
        "schema = dict(schema)\n",
        "#remove = pd.read_csv('/content/RemoveRowsCount.csv',header=None).values.tolist()\n",
        "#remove = [i[0] for i in remove]\n",
        "#removerows = ['id','member_id','url','desc','zip_code','addr_state']\n",
        "#removerows.extend(remove)\n",
        "#print(removerows)\n",
        "#for i in removerows:\n",
        "#  df = df.drop(i)\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "df = df.withColumn(\"id\", monotonically_increasing_id())\n",
        "\n",
        "columns = df.columns\n",
        "norows = df.count()\n",
        "types = [f.dataType for f in df.schema.fields]\n",
        "typ = {columns[i]: types[i] for i in range(len(types))}\n",
        "strcolm = []\n",
        "columns = df.columns\n",
        "for col in columns:\n",
        "  if str(typ[col]) == 'StringType':\n",
        "    strcolm.append(col)\n",
        " \n",
        "for col in schema:\n",
        "  if col in strcolm:\n",
        "    if schema[col]== 'date':\n",
        "      df = df.withColumn(col, f.to_timestamp(df[col], 'MMM-yyyy'))\n",
        "    elif schema[col]== 'int':\n",
        "      df = df.withColumn(col, df[col].cast(IntegerType()))\n",
        "    elif schema[col] == 'double':\n",
        "      df = df.withColumn(col, df[col].cast(DoubleType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0de9x-Xm7_C4"
      },
      "source": [
        "##Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNPpYaYB71AL"
      },
      "source": [
        "pastdf = df.filter((df.loan_status != 'Current')&(df.loan_status != 'Does not meet the credit policy. Status:Charged Off')&(df.loan_status != 'Does not meet the credit policy. Status:Fully Paid'))\n",
        "zip= {}\n",
        "value = {}\n",
        "zip['emp_length']= [' reactors\"']\n",
        "zip['home_ownership']=['2 years']\n",
        "zip['verification_status'] =['38000']\n",
        "zip['loan_status'] = ['01-10-2015']\n",
        "value['application_type'] = ['Individual','Joint App']\n",
        "value['initial_list_status'] = ['W','F']\n",
        "\n",
        "for i in zip:\n",
        "  for j in zip[i]:\n",
        "    pastdf = pastdf.withColumn(i, f.when(pastdf[i] == j,'').otherwise(pastdf[i]))\n",
        "for i in value:\n",
        "  for j in value[i]:\n",
        "    pastdf = pastdf.withColumn(i, f.when(pastdf[i] == j,pastdf[i]).otherwise(''))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtXvh2sR8G5U"
      },
      "source": [
        "##Creating Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIUa592a7v7z"
      },
      "source": [
        "x=25\n",
        "pastdf = pastdf.withColumn('amnt_left_per', (f.col('total_rec_prncp')/(f.col('funded_amnt')))*100)\n",
        "pastdf = pastdf.withColumn('target',f.when(f.col('loan_status')=='Fully Paid',0).otherwise(f.when(f.col('amnt_left_per')>x,2).otherwise(1)))\n",
        "pastdf = pastdf.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHpMH_QR8YXu"
      },
      "source": [
        "#Outlier Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MaG7srF8buf"
      },
      "source": [
        "##z-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKAilYJ88X3U"
      },
      "source": [
        "from scipy.stats import zscore\n",
        "for col in pdf.columns:\n",
        "  if pdf[col].dtype == 'float64':\n",
        "    m = pdf[col].mean()\n",
        "  pdf['z'] = zscore(pdf[col])\n",
        "  pdf.loc[pdf['z'].abs()>3,col] = m\n",
        "pdf = pdf.drop('z',axis =1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOtWlZaL8dpj"
      },
      "source": [
        "##DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y53Xwzyj_ct4"
      },
      "source": [
        "### For Class 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYZTEsuc8fjO"
      },
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "reg = pdf.select_dtypes(['float64']).columns\n",
        "X = pdf[pdf.target==1]\n",
        "X = X[reg]\n",
        "ss = MinMaxScaler()\n",
        "X = ss.fit_transform(X)\n",
        "X[np.isnan(X)]=-1\n",
        "db = DBSCAN(eps=0.5, min_samples=5,n_jobs=8)\n",
        "db.fit(X)\n",
        "remove = db.fit_predict(X)\n",
        "li = np.where(remove==-1)[0].tolist()\n",
        "t = pdf[pdf.target==1].reset_index()\n",
        "re = t.iloc[li]\n",
        "re['index'].to_excel('RemoveDBScan1.xlsx',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr8AQvT-AWwP"
      },
      "source": [
        "### For Class 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZInhXYssAV9I"
      },
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "reg = pdf.select_dtypes(['float64']).columns\n",
        "X = pdf[pdf.target==2]\n",
        "X = X[reg]\n",
        "ss = MinMaxScaler()\n",
        "X = ss.fit_transform(X)\n",
        "X[np.isnan(X)]=-1\n",
        "db = DBSCAN(eps=0.5, min_samples=5,n_jobs=8)\n",
        "db.fit(X)\n",
        "remove = db.fit_predict(X)\n",
        "li = np.where(remove==-1)[0].tolist()\n",
        "t = pdf[pdf.target==2].reset_index()\n",
        "re = t.iloc[li]\n",
        "re['index'].to_excel('RemoveDBScan2.xlsx',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwiGNyzsA_xr"
      },
      "source": [
        "#Combining RemoveDBScan1.xlsx & RemoveDBScan2.xlsx into single DBRemove.csv file manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXjAaK4q6ehC"
      },
      "source": [
        "#Deriving Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6jAn98B6Lm0"
      },
      "source": [
        "##IV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16OG5k7D6Kq5"
      },
      "source": [
        "from pyspark.ml.feature import QuantileDiscretizer\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql.functions import log\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql import Window \n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def final_iv(df,target):\n",
        "  columns = df.columns\n",
        "  error ={}\n",
        "  columns.remove(target)\n",
        "  types = [f.dataType for f in df.schema.fields]\n",
        "  typ = {columns[i]: types[i] for i in range(len(types)-1)}\n",
        "  d = pd.DataFrame({},index=[])\n",
        "  for col in tqdm(columns):\n",
        "    try:\n",
        "      print(str(typ[col])+' '+col)\n",
        "      pdf = df.select(df[col],df[target])\n",
        "      #.filter(df[col].isNotNull())\n",
        "      #summ = pdf.select(pdf[col]).summary()\n",
        "      #pdf.groupby(col).count().show()\n",
        "      if pdf.count() == 0:\n",
        "        error[col] = ['Null Columns']\n",
        "        continue\n",
        "      if str(typ[col]) != 'DateType' and str(typ[col]) != 'StringType':\n",
        "        pdf = pdf.filter(pdf[col]!= 0.0)\n",
        "        discretizer = QuantileDiscretizer(numBuckets=20, inputCol=col, outputCol=\"buckets\")\n",
        "        pdf = discretizer.fit(pdf).transform(pdf)\n",
        "        pdf = pdf.groupby(target,'buckets').count().select(target,f.col('buckets').alias('Value'),f.col('count').alias('freq'))\n",
        "      else:\n",
        "        pdf = pastdf.select(f.col(col).alias('Value'),target).groupBy(target,'Value').count().select(target,'Value', f.col('count').alias('freq'))\n",
        "        if pdf.count() > 20:\n",
        "          error[col] = ['Too many columns']\n",
        "          continue\n",
        "      pdf = pdf.withColumn('percent',f.col('freq')/f.sum('freq').over(Window.partitionBy(target)))\n",
        "      event = pdf.filter(pdf[target] == 1).drop(target).select('Value',f.col('freq').alias('Event'),f.col('percent').alias('Event%'))\n",
        "      nonevent = pdf.filter(pdf[target] == 0).drop(target).select('Value',f.col('freq').alias('Non-Event'),f.col('percent').alias('Non-Event%'))\n",
        "      inner_join = event.join(nonevent,on=['Value'],how='outer')\n",
        "      inner_join = inner_join.withColumn('WOE',log(f.col('Non-Event%')/f.col('Event%'))).withColumn('Non-Event%-Event%',f.col('Non-Event%')-f.col('Event%')).withColumn('IV',f.col('Non-Event%-Event%')*f.col('WOE'))\n",
        "      dft =inner_join.withColumn('Variable',lit(col)).toPandas()\n",
        "      dft.loc['Column_Total']= dft.sum(numeric_only=True, axis=0)\n",
        "      d = d.append(dft)\n",
        "    except Exception as e:\n",
        "     error[col] = [str(e)]\n",
        "  return d,error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9tLVjhY6uiY"
      },
      "source": [
        "# d has IV\n",
        "# e is error variables with reason\n",
        "d,e = final_iv(pastdf,'target')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vtYVuXJ8p7J"
      },
      "source": [
        "##IV Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE7bOuc58sVz"
      },
      "source": [
        "def calc_iv(df, feature, target, pr=False):\n",
        "    \"\"\"\n",
        "    Set pr=True to enable printing of output.\n",
        "    \n",
        "    Output: \n",
        "      * iv: float,\n",
        "      * data: pandas.DataFrame\n",
        "    \"\"\"\n",
        "    #return feature + str(df[feature].dtype)\n",
        "    lst = []\n",
        "    if (str(df[feature].dtype) == 'float64') or (str(df[feature].dtype) == 'int64'):\n",
        "      #print(feature)\n",
        "      #print(str(df[feature].dtype))\n",
        "      #df[feature] = df[feature].fillna(-999999999)\n",
        "      l = len(pd.qcut(df[feature],q=5,duplicates='drop').value_counts())\n",
        "      df[feature] = pd.qcut(df[feature],labels=np.arange(l) ,q=5,duplicates='drop')\n",
        "    else:\n",
        "      if len(df[feature].unique())>=20:\n",
        "        return -1,-1\n",
        "      #else:\n",
        "        #df[feature] = df[feature].fillna(\"\")\n",
        "    for i in range(df[feature].nunique()):\n",
        "        val = list(df[feature].unique())[i]\n",
        "        lst.append([feature,                                                        # Variable\n",
        "                    val,                                                            # Value\n",
        "                    df[df[feature] == val].count()[feature],                        # All\n",
        "                    df[(df[feature] == val) & (df[target] == 0)].count()[feature],  # Good (think: Fraud == 0)\n",
        "                    df[(df[feature] == val) & (df[target] != 0)].count()[feature]]) # Bad (think: Fraud == 1)\n",
        "\n",
        "    data = pd.DataFrame(lst, columns=['Variable', 'Value', 'All', 'Good', 'Bad'])\n",
        "\n",
        "    data['Share'] = data['All'] / data['All'].sum()\n",
        "    data['Bad Rate'] = data['Bad'] / data['All']\n",
        "    data['Distribution Good'] = (data['All'] - data['Bad']) / (data['All'].sum() - data['Bad'].sum())\n",
        "    data['Distribution Bad'] = data['Bad'] / data['Bad'].sum()\n",
        "    data['WoE'] = np.log(data['Distribution Good'] / data['Distribution Bad'])\n",
        "\n",
        "    data = data.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n",
        "\n",
        "    data['IV'] = data['WoE'] * (data['Distribution Good'] - data['Distribution Bad'])\n",
        "\n",
        "    data = data.sort_values(by=['Variable', 'Value'], ascending=[True, True])\n",
        "    data.index = range(len(data.index))\n",
        "\n",
        "    if pr:\n",
        "        print(data)\n",
        "        print('IV = ', data['IV'].sum())\n",
        "\n",
        "\n",
        "    iv = data['IV'].sum()\n",
        "    # print(iv)\n",
        "\n",
        "    return iv, data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EECqCo0y8yHm"
      },
      "source": [
        "from tqdm import tqdm\n",
        "columns = pdf.columns.to_list()\n",
        "j=0\n",
        "columns.remove('target')\n",
        "iv={}\n",
        "data = pd.DataFrame(index=None)\n",
        "for col in tqdm(pdf.columns):\n",
        "  #print(calc_iv(pdf[['target',col]],col,'target'))\n",
        "  i,d = calc_iv(pdf[['target',col]],feature=col,target='target')\n",
        "  iv[col] = i\n",
        "  if i == -1:\n",
        "    j +=1\n",
        "    continue\n",
        "  else:\n",
        "    data =data.append(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSrKC9kj9B9u"
      },
      "source": [
        "#IV of -1 is when there are too many groups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRQQ2fTw83zc"
      },
      "source": [
        "pd.DataFrame(iv.items()).to_excel('IV_pandas.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJKRn-z76YIL"
      },
      "source": [
        "##Gini"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05VbZUjc6Xbc"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def giniIndex(df,target):\n",
        "  gicols = {}\n",
        "  error={}\n",
        "  columns = df.columns\n",
        "  types = [f.dataType for f in df.schema.fields]\n",
        "  typ = {columns[i]: types[i] for i in range(len(types))}\n",
        "  columns.remove(target)\n",
        "  #tar = df.select(target)\n",
        "  #tar = np.array(tar.collect())\n",
        "  #tar[tar==2]=1\n",
        "  for col in tqdm(columns):\n",
        "    print(col)\n",
        "    if str(typ[col]) not in ['StringType','DateType']:\n",
        "      tar = df.select(col,target).toPandas().dropna()\n",
        "      tar[tar==2]=1\n",
        "      pred = tar.pop(col)\n",
        "      #pred =np.array(pred.collect())\n",
        "      fpr, tpr, thresholds = metrics.roc_curve(tar, pred)\n",
        "      auc = metrics.auc(fpr, tpr)\n",
        "      gini = 2*auc -1\n",
        "      #print(gicols[col])\n",
        "      gicols[col] =  gini  \n",
        "    else:\n",
        "      gicols[col]= typ[col]\n",
        "  return (gicols,error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06Lg3Fvl6ryD"
      },
      "source": [
        "giniIndex(pastdf,'target')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcsqX3ya7C6T"
      },
      "source": [
        "##Correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgvOMUMS7Eq_"
      },
      "source": [
        "from tqdm import tqdm\n",
        "col = pastdf.columns[0]\n",
        "for col in tqdm(pastdf.columns):\n",
        "  crs ={}\n",
        "  if str(typ[col]) not in ['StringType','DateType'] and len(cross[col])!=0: \n",
        "    for c in tqdm(cross[col]):\n",
        "      if str(typ[c]) not in ['StringType','DateType']:\n",
        "        crs[c] = pastdf.stat.corr(c,col)\n",
        "    cross[col] = crs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IswyADar7R0Z"
      },
      "source": [
        "pd.DataFrame.from_dict(cross,orient='index').to_excel('Covariance.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOHtDj4mBYPw"
      },
      "source": [
        "# After seeing Importance of Variables csv of dropped variables named as 'FinalDrop.csv' is compiled manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m1V3DJX7m_x"
      },
      "source": [
        "#Export Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGRfqW0B7lVW"
      },
      "source": [
        "pastdf.write.format('csv').option('header',False).mode('overwrite').option('sep',',').save('/content/drive/My Drive/Lending-Club/loanFinal.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhF2zuAxDk1M"
      },
      "source": [
        "#Clean Data Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1tTFoDeDf-P"
      },
      "source": [
        "pdf = pd.read_csv('/content/drive/My Drive/Lending-Club/loanFinal.csv', header=0, escapechar='\\\\')\n",
        "dropcol = pd.read_csv('/content/drive/My Drive/Lending-Club/FinalDrop.csv',header=None)[0].to_list()\n",
        "dbindex = pd.read_csv('/content/drive/My Drive/Lending-Club/DBRemove.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "029rT0zWEEo3"
      },
      "source": [
        "df = spark.createDataFrame(pdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0axodfaDvQP"
      },
      "source": [
        "from tqdm import tqdm \n",
        "stats = {}\n",
        "for col in tqdm(df.columns):\n",
        "  stats[col] = df.select(col).summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDSus3nQDvQ1"
      },
      "source": [
        "p = pd.DataFrame(index=None,columns=[])\n",
        "norows = df.count()\n",
        "for col in stats:\n",
        "  pdf = stats[col].toPandas()\n",
        "  pdf_T = pdf.T\n",
        "  pdf_T.columns = pdf_T.iloc[0]\n",
        "  pdf_T = pdf_T[1:]\n",
        "  p=p.append(pdf_T)\n",
        "p['fill%'] = (p['count'].astype(int)/norows)*100\n",
        "p.to_excel(\"CleanDataStatistics.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}