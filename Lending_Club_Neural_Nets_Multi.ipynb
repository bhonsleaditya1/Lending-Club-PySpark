{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lending-Club-Neural-Nets-Multi.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "S3BPY7q8E7aU",
        "2SZbbJMKFBwG"
      ],
      "authorship_tag": "ABX9TyPRDcGbz1kYmJ/hxofcFA3L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhonsleaditya1/Lending-Club-PySpark/blob/master/Lending_Club_Neural_Nets_Multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3BPY7q8E7aU"
      },
      "source": [
        "#Installing & Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3iavbVL1S4h",
        "outputId": "6be52c38-3053-470e-d125-2c3e39c5c46d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "!pip install h5py\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score,confusion_matrix,classification_report,roc_curve,auc\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.optimizers import schedules\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout\n",
        "from keras.utils import np_utils,to_categorical"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SZbbJMKFBwG"
      },
      "source": [
        "#Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53tFZKBs1hzG",
        "outputId": "a493781e-b3fb-4a3e-cbaf-f1d338f167a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "pdf = pd.read_csv('/content/drive/My Drive/Lending-Club/loanFinal.csv', header=0, escapechar='\\\\')\n",
        "dropcol = pd.read_csv('/content/drive/My Drive/Lending-Club/FinalDrop.csv',header=None)[0].to_list()\n",
        "dbindex = pd.read_csv('/content/drive/My Drive/Lending-Club/DBRemove.csv')\n",
        "pdf = pdf[pdf.amnt_left_per<=100]\n",
        "#pdf = pdf[pdf.amnt_left_per>=0]\n",
        "pdf = pdf.drop(dropcol,axis=1)\n",
        "#pdf=pdf[pdf['application_type'].isin(['Joint App','Individual'])]\n",
        "#pdf=pdf[pdf['initial_list_status'].isin(['w','f'])]\n",
        "pdf = pdf.drop(dbindex['index'].to_list())\n",
        "pdf = pdf.reset_index().drop('index',axis=1)\n",
        "pdf.count()\n",
        "pdf['year'] = pd.DatetimeIndex(pdf['issue_d']).year\n",
        "#pdf = pdf.drop(dt,axis=1)\n",
        "reg = pdf.select_dtypes(['float64']).columns\n",
        "clas = pdf.select_dtypes(['O']).columns\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (41,49,117,118,119,122,123,124,127) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3LOSlgq2ip3",
        "outputId": "221a7007-bda5-4770-a4cb-3a5b6a25468d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "del pdf\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTsn83ZtFGZP"
      },
      "source": [
        "#Data Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pftOcrD0FI-K"
      },
      "source": [
        "##OneHotEncoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgfL51KV11vQ",
        "outputId": "91d1feff-3e9a-442c-b75e-ee101fe1a2a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#OneHotEncoding Multi\n",
        "p = pd.DataFrame(index=None)\n",
        "for i in clas:\n",
        "  if pdf[i].nunique()<25:\n",
        "    l = pd.get_dummies(pdf[i], prefix=i)\n",
        "    for j in l.columns:\n",
        "      pdf[j] = l[j]\n",
        "for i in clas:\n",
        "  pdf = pdf.drop(i,axis=1)\n",
        "pdf = pdf.rename(columns={'emp_length_< 1 year':'emp_length_less 1 year'})\n",
        "timetest = pdf[pdf.year==2016]\n",
        "train = pdf[pdf.year!=2016]\n",
        "timetest = timetest.drop(['year'],axis=1)\n",
        "train = train.drop(['year'],axis=1)\n",
        "y_timetest=pd.DataFrame()\n",
        "y_t = pd.DataFrame()\n",
        "y_timetest['target'] = timetest['target']\n",
        "y_t['target'] = train['target']\n",
        "timetest = timetest.drop('target',axis=1)\n",
        "train = train.drop('target',axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(train, y_t, random_state=1301, test_size=0.3)\n",
        "scale = StandardScaler()\n",
        "imp = SimpleImputer(strategy=\"mean\")\n",
        "scale.fit(imp.fit_transform(X_train))\n",
        "X_train = scale.transform(imp.fit_transform(X_train))\n",
        "X_test = scale.transform(imp.fit_transform(X_test))\n",
        "timetest = scale.transform(imp.fit_transform(timetest))\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmSNuB8tyd0n"
      },
      "source": [
        "#From this Cell\n",
        "1.   0 is Class 0\n",
        "2.   1 is Class 1\n",
        "3.   2 is Class 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KaxvhQ3FL7O"
      },
      "source": [
        "#Base Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "981nmVAbFP_5"
      },
      "source": [
        "##MLP Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1Rk6y0p267i"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(256, 256), max_iter=150,activation='relu',solver='adam',learning_rate='adaptive',verbose=True)\n",
        "mlp.fit(X_train, y_train.values.ravel())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1_w7FmgFUbj"
      },
      "source": [
        "###Saving Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG28vigfcdIm",
        "outputId": "718ba878-f523-444c-ea60-91258bee5397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import joblib\n",
        "joblib.dump(mlp, '/content/drive/My Drive/Lending-Club/Models/NN_Multi_mlp.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Lending-Club/Models/NN_1_mlp.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJyeo8cTFkhd"
      },
      "source": [
        "##Optimized Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0mAeIDHlmG3"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed()\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import log_loss, roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Dropout, Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, Callback\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "class roc_auc_callback(Callback):\n",
        "    def __init__(self,training_data,validation_data):\n",
        "        self.x = training_data[0]\n",
        "        self.y = training_data[1]\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        y_pred = self.model.predict_proba(self.x, verbose=0)\n",
        "        roc = roc_auc_score(self.y, y_pred)\n",
        "        logs['roc_auc'] = roc_auc_score(self.y, y_pred)\n",
        "        logs['norm_gini'] = ( roc_auc_score(self.y, y_pred) * 2 ) - 1\n",
        "\n",
        "        y_pred_val = self.model.predict_proba(self.x_val, verbose=0)\n",
        "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "        logs['roc_auc_val'] = roc_auc_score(self.y_val, y_pred_val)\n",
        "        logs['norm_gini_val'] = ( roc_auc_score(self.y_val, y_pred_val) * 2 ) - 1\n",
        "\n",
        "        print('\\rroc_auc: %s - roc_auc_val: %s - norm_gini: %s - norm_gini_val: %s' % (str(round(roc,5)),str(round(roc_val,5)),str(round((roc*2-1),5)),str(round((roc_val*2-1),5))), end=10*' '+'\\n')\n",
        "        return\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "folds = 4\n",
        "runs = 2\n",
        "\n",
        "cv_LL = 0\n",
        "cv_AUC = 0\n",
        "cv_gini = 0\n",
        "fpred = []\n",
        "avpred = []\n",
        "avreal = []\n",
        "avids = []\n",
        "patience = 10\n",
        "batchsize = 2048\n",
        "\n",
        "def timer(start_time=None):\n",
        "    if not start_time:\n",
        "        start_time = datetime.now()\n",
        "        return start_time\n",
        "    elif start_time:\n",
        "        thour, temp_sec = divmod(\n",
        "            (datetime.now() - start_time).total_seconds(), 3600)\n",
        "        tmin, tsec = divmod(temp_sec, 60)\n",
        "        print('\\n Time taken: %i hours %i minutes and %s seconds.' %\n",
        "              (thour, tmin, round(tsec, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1zSA5yHlqP7"
      },
      "source": [
        "import keras\n",
        "# Let's split the data into folds. I always use the same random number for reproducibility, \n",
        "# and suggest that you do the same (you certainly don't have to use 1001).\n",
        "\n",
        "#skf = StratifiedKFold(n_splits=folds, random_state=1001)\n",
        "#starttime = timer(None)\n",
        "#for i, (train_index, test_index) in enumerate(skf.split(X_train,y_train)):\n",
        "#    start_time = timer(None)\n",
        "#    X1, X_val1 = X_train[train_index], X_train[test_index]\n",
        "#    y1, y_val1 = y_train[train_index], y_train[test_index]\n",
        "    #train_ids, val_ids = tr_ids[train_index], tr_ids[test_index]\n",
        "    \n",
        "# This is where we define and compile the model. These parameters are not optimal, as they were chosen \n",
        "# to get a notebook to complete in 60 minutes. Other than leaving BatchNormalization and last sigmoid \n",
        "# activation alone, virtually everything else can be optimized: number of neurons, types of initializers, \n",
        "# activation functions, dropout values. The same goes for the optimizer at the end.\n",
        "\n",
        "#########\n",
        "# Never move this model definition to the beginning of the file or anywhere else outside of this loop. \n",
        "# The model needs to be initialized anew every time you run a different fold. If not, it will continue \n",
        "# the training from a previous model, and that is not what you want.\n",
        "#########\n",
        "\n",
        "    # This definition must be within the for loop or else it will continue training previous model\n",
        "def baseline_model():\n",
        "    model = Sequential()\n",
        "    model.add(\n",
        "        Dense(\n",
        "            256,\n",
        "            input_dim=X_train.shape[1],\n",
        "            kernel_initializer='glorot_normal',\n",
        "            ))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, kernel_initializer='glorot_normal'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.25))\n",
        "    #model.add(Dense(50, kernel_initializer='glorot_normal'))\n",
        "    #model.add(Activation('relu'))\n",
        "    #model.add(BatchNormalization())\n",
        "    #model.add(Dropout(0.15))\n",
        "    #model.add(Dense(25, kernel_initializer='glorot_normal'))\n",
        "    #model.add(Activation('relu'))\n",
        "    #model.add(BatchNormalization())\n",
        "    #model.add(Dropout(0.1))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer='adagrad', metrics = [keras.metrics.AUC()], loss='binary_crossentropy')\n",
        "\n",
        "    return model\n",
        "\n",
        "# This is where we repeat the runs for each fold. If you choose runs=1 above, it will run a \n",
        "# regular N-fold procedure.\n",
        "\n",
        "#########\n",
        "# It is important to leave the call to random seed here, so each run starts with a different seed.\n",
        "#########\n",
        "\n",
        "#for run in range(runs):\n",
        "#  print('\\n Fold %d - Run %d\\n' % ((i + 1),(run + 1)))\n",
        "#  np.random.seed()\n",
        "\n",
        "# Lots to unpack here.\n",
        "\n",
        "# The first callback prints out roc_auc and gini values at the end of each epoch. It must be listed \n",
        "# before the EarlyStopping callback, which monitors gini values saved in the previous callback. Make \n",
        "# sure to set the mode to \"max\" because the default value (\"auto\") will not handle gini properly \n",
        "# (it will act as if the model is not improving even when roc/gini go up).\n",
        "\n",
        "# CSVLogger creates a record of all iterations. Not really needed but it doesn't hurt to have it.\n",
        "\n",
        "# ModelCheckpoint saves a model each time gini improves. Its mode also must be set to \"max\" for reasons \n",
        "# explained above.\n",
        "\n",
        "callbacks = [\n",
        "    roc_auc_callback(training_data=(X_train, to_categorical(y_train)),validation_data=(X_test, to_categorical(y_test))),  # call this before EarlyStopping\n",
        "    EarlyStopping(monitor='norm_gini_val', patience=patience, mode='max', verbose=1),\n",
        "    CSVLogger('keras-5fold-run-01-v1-epochs.log', separator=',', append=False),\n",
        "    ModelCheckpoint(\n",
        "            'keras-5fold-run-01-v1-fold-' + str('%02d' % (0 + 1)) + '-run-' + str('%02d' % (0 + 1)) + 'Multi.check',\n",
        "            monitor='norm_gini_val', mode='max', # mode must be set to max or Keras will be confused\n",
        "            save_best_only=True,\n",
        "            verbose=1)\n",
        "]\n",
        "\n",
        "# The classifier is defined here. Epochs should be be set to a very large number (not 3 like below) which \n",
        "# will never be reached anyway because of early stopping. I usually put 5000 there. Because why not.\n",
        "\n",
        "nnet = KerasClassifier(\n",
        "    build_fn=baseline_model,\n",
        "# Epoch needs to be set to a very large number ; early stopping will prevent it from reaching\n",
        "#            epochs=5000,\n",
        "    epochs=500,\n",
        "    batch_size=batchsize,\n",
        "    validation_data=(X_test, to_categorical(y_test)),\n",
        "    verbose=2,\n",
        "    shuffle=True,\n",
        "    callbacks=callbacks)\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train.target)\n",
        "\n",
        "fit = nnet.fit(X_train,to_categorical(y_train),class_weight=dict(zip(np.unique(y_train),class_weights)))\n",
        "        \n",
        "# We want the best saved model - not the last one where the training stopped. So we delete the old \n",
        "# model instance and load the model from the last saved checkpoint. Next we predict values both for \n",
        "# validation and test data, and create a summary of parameters for each run.\n",
        "\n",
        "del nnet\n",
        "nnet = load_model('keras-5fold-run-01-v1-fold-' + str('%02d' % (0 + 1)) + '-run-' + str('%02d' % (0 + 1)) + 'Multi.check')\n",
        "scores_val_run = nnet.predict_proba(X_test, verbose=0)\n",
        "LL_run = log_loss(X_test, scores_val_run)\n",
        "print('\\n Fold %d Run %d Log-loss: %.5f' % ((i + 1), (run + 1), LL_run))\n",
        "AUC_run = roc_auc_score(to_categorical(y_test), scores_val_run)\n",
        "print(' Fold %d Run %d AUC: %.5f' % ((i + 1), (run + 1), AUC_run))\n",
        "print(' Fold %d Run %d normalized gini: %.5f' % ((i + 1), (run + 1), AUC_run*2-1))\n",
        "y_pred_run = nnet.predict_proba(timetest, verbose=0)\n",
        "if run > 0:\n",
        "    scores_val = scores_val + scores_val_run\n",
        "    y_pred = y_pred + y_pred_run\n",
        "else:\n",
        "    scores_val = scores_val_run\n",
        "    y_pred = y_pred_run\n",
        "            \n",
        "# We average all runs from the same fold and provide a parameter summary for each fold. Unless something \n",
        "# is wrong, the numbers printed here should be better than any of the individual runs.\n",
        "\n",
        "scores_val = scores_val / runs\n",
        "y_pred = y_pred / runs\n",
        "LL = log_loss(y_val, scores_val)\n",
        "print('\\n Fold %d Log-loss: %.5f' % ((i + 1), LL))\n",
        "AUC = roc_auc_score(y_val, scores_val)\n",
        "print(' Fold %d AUC: %.5f' % ((i + 1), AUC))\n",
        "print(' Fold %d normalized gini: %.5f' % ((i + 1), AUC*2-1))\n",
        "timer(start_time)\n",
        "\n",
        "# We add up predictions on the test data for each fold. Create out-of-fold predictions for validation data.\n",
        "\n",
        "if i > 0:\n",
        "    fpred = pred + y_pred\n",
        "    avreal = np.concatenate((avreal, y_val), axis=0)\n",
        "    avpred = np.concatenate((avpred, scores_val), axis=0)\n",
        "    avids = np.concatenate((avids, val_ids), axis=0)\n",
        "else:\n",
        "    fpred = y_pred\n",
        "    avreal = y_val\n",
        "    avpred = scores_val\n",
        "    avids = val_ids\n",
        "pred = fpred\n",
        "cv_LL = cv_LL + LL\n",
        "cv_AUC = cv_AUC + AUC\n",
        "cv_gini = cv_gini + (AUC*2-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X06B6OekF8OS"
      },
      "source": [
        "###Metrics for best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07kTLuDpKXmh"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(\n",
        "    Dense(\n",
        "        256,\n",
        "        input_dim=X_train.shape[1],\n",
        "        kernel_initializer='glorot_normal',\n",
        "        ))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, kernel_initializer='glorot_normal'))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "#lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-6,decay_steps=10000,decay_rate=0.9)\n",
        "#sgd = keras.optimizers.Adagrad(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=[keras.metrics.AUC()])\n",
        "model.load_weights('keras-5fold-run-01-v1-fold-' + str('%02d' % (0 + 1)) + '-run-' + str('%02d' % (0 + 1)) + 'Multi.check')\n",
        "nnet=model\n",
        "#model.load_weights('keras-5fold-run-01-v1-fold-' + str('%02d' % (i + 1)) + '-run-' + str('%02d' % (run + 1)) + '.check')\n",
        "scores_val_run = nnet.predict_proba(X_test, verbose=0)\n",
        "LL_run = log_loss(X_test, scores_val_run)\n",
        "print('\\n Fold %d Run %d Log-loss: %.5f' % ((i + 1), (run + 1), LL_run))\n",
        "AUC_run = roc_auc_score(to_categorical(y_test), scores_val_run)\n",
        "print(' Fold %d Run %d AUC: %.5f' % ((i + 1), (run + 1), AUC_run))\n",
        "print(' Fold %d Run %d normalized gini: %.5f' % ((i + 1), (run + 1), AUC_run*2-1))\n",
        "y_pred_run = nnet.predict_proba(timetest, verbose=0)\n",
        "if run > 0:\n",
        "    scores_val = scores_val + scores_val_run\n",
        "    y_pred = y_pred + y_pred_run\n",
        "else:\n",
        "    scores_val = scores_val_run\n",
        "    y_pred = y_pred_run\n",
        "            \n",
        "# We average all runs from the same fold and provide a parameter summary for each fold. Unless something \n",
        "# is wrong, the numbers printed here should be better than any of the individual runs.\n",
        "\n",
        "scores_val = scores_val / runs\n",
        "y_pred = y_pred / runs\n",
        "LL = log_loss(y_val, scores_val)\n",
        "print('\\n Fold %d Log-loss: %.5f' % ((i + 1), LL))\n",
        "AUC = roc_auc_score(y_val, scores_val)\n",
        "print(' Fold %d AUC: %.5f' % ((i + 1), AUC))\n",
        "print(' Fold %d normalized gini: %.5f' % ((i + 1), AUC*2-1))\n",
        "timer(start_time)\n",
        "\n",
        "# We add up predictions on the test data for each fold. Create out-of-fold predictions for validation data.\n",
        "\n",
        "if i > 0:\n",
        "    fpred = pred + y_pred\n",
        "    avreal = np.concatenate((avreal, y_val), axis=0)\n",
        "    avpred = np.concatenate((avpred, scores_val), axis=0)\n",
        "    avids = np.concatenate((avids, val_ids), axis=0)\n",
        "else:\n",
        "    fpred = y_pred\n",
        "    avreal = y_val\n",
        "    avpred = scores_val\n",
        "    avids = val_ids\n",
        "pred = fpred\n",
        "cv_LL = cv_LL + LL\n",
        "cv_AUC = cv_AUC + AUC\n",
        "cv_gini = cv_gini + (AUC*2-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06CeZ7EYGJse"
      },
      "source": [
        "##SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78L2J44x38sn"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.optimizers import schedules\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout\n",
        "from keras.utils import np_utils,to_categorical\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train.target)\n",
        "class_weights=dict(zip(np.unique(y_train),class_weights))\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-6,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "sgd = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[keras.metrics.AUC()])\n",
        "model.fit(X_train, np_utils.to_categorical(y_train),validation_data=(X_test,to_categorical(y_test)), epochs=200, batch_size=1024,use_multiprocessing=True,shuffle=True,class_weight=class_weights)\n",
        "#Saving model\n",
        "model.save_weights(\"/content/drive/My Drive/Lending-Club/Models/NN_Multi_SGD_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d6D1lVRGQN6"
      },
      "source": [
        "##SGD Nesterov"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYRTDZOMGPM0"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.optimizers import schedules\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout\n",
        "from keras.utils import np_utils,to_categorical\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train.target)\n",
        "class_weights=dict(zip(np.unique(y_train),class_weights))\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-6,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "sgd = keras.optimizers.SGD(learning_rate=lr_schedule,nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[keras.metrics.AUC()])\n",
        "model.fit(X_train, np_utils.to_categorical(y_train),validation_data=(X_test,to_categorical(y_test)), epochs=200, batch_size=1024,use_multiprocessing=True,shuffle=True,class_weight=class_weights)\n",
        "#Saving model\n",
        "model.save_weights(\"/content/drive/My Drive/Lending-Club/Models/NN_Multi_SGD_N_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1exfgd5Gfnn"
      },
      "source": [
        "##Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXKIONx3GeAS"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.optimizers import schedules\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout\n",
        "from keras.utils import np_utils,to_categorical\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train.target)\n",
        "class_weights=dict(zip(np.unique(y_train),class_weights))\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-6,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "sgd = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[keras.metrics.AUC()])\n",
        "model.fit(X_train, np_utils.to_categorical(y_train),validation_data=(X_test,to_categorical(y_test)), epochs=200, batch_size=1024,use_multiprocessing=True,shuffle=True,class_weight=class_weights)\n",
        "#Saving model\n",
        "model.save_weights(\"/content/drive/My Drive/Lending-Club/Models/NN_Multi_Adam_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QH5m3yZIVmG"
      },
      "source": [
        "##Keras Bayesian Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohkcka1bIlNe"
      },
      "source": [
        "###Trial Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpuw0WITU6Pa",
        "outputId": "d4e8ed66-ef43-48d6-895f-0dcc0049b41f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "!pip install -q keras-tuner\n",
        "from kerastuner import HyperModel\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.optimizers import schedules\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout\n",
        "from tensorflow.keras import utils\n",
        "from keras.utils import np_utils,to_categorical\n",
        "\n",
        "class ClassificationHyperModel(HyperModel):\n",
        "    def __init__(self, input_shape):\n",
        "      self.input_shape = input_shape\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = tensorflow.keras.Sequential()\n",
        "\n",
        "        model.add(\n",
        "            layers.Dense(\n",
        "                units=hp.Choice('units', values=[64,128,256,512], default=128),\n",
        "                activation=hp.Choice(\n",
        "                    'dense_activation',\n",
        "                    values=['relu', 'tanh', 'sigmoid'],\n",
        "                    default='relu'),\n",
        "                input_shape=input_shape\n",
        "            )\n",
        "        )\n",
        "        model.add(\n",
        "            layers.Dropout(\n",
        "                hp.Float(\n",
        "                    'dropout',\n",
        "                    min_value=0.2,\n",
        "                    max_value=0.5,\n",
        "                    default=0.25,\n",
        "                    step=0.1)\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        model.add(\n",
        "            layers.Dense(\n",
        "                units=hp.Choice('units', values=[64,128,256,512], default=128),\n",
        "                activation=hp.Choice(\n",
        "                    'dense_activation',\n",
        "                    values=['relu', 'tanh', 'sigmoid'],\n",
        "                    default='relu')\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        model.add(\n",
        "            layers.Dropout(\n",
        "                hp.Float(\n",
        "                    'dropout',\n",
        "                    min_value=0.2,\n",
        "                    max_value=0.5,\n",
        "                    default=0.25,\n",
        "                    step=0.1)\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        model.add(layers.Dense(3,activation='softmax'))\n",
        "        \n",
        "        model.compile(\n",
        "            hp.Choice('optimizer',values=['Adagrad','Sgd','Adam'],default='Adam')\n",
        "            ,loss='categorical_crossentropy',metrics=[keras.metrics.AUC()]\n",
        "        )\n",
        "        \n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██████                          | 10kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.5MB/s \n",
            "\u001b[?25h  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CjA3dctIn_P"
      },
      "source": [
        "###Run Trials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lO0qlJyVBsh"
      },
      "source": [
        "import kerastuner\n",
        "from tensorflow import keras\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train.target)\n",
        "class_weights=dict(zip(np.unique(y_train),class_weights))\n",
        "input_shape = (X_train.shape[1],)\n",
        "hypermodel = ClassificationHyperModel(input_shape)\n",
        "tuner_bo = kerastuner.tuners.BayesianOptimization(\n",
        "            hypermodel,\n",
        "            objective=kerastuner.Objective(\"auc\", direction=\"max\"),\n",
        "            max_trials=2,\n",
        "            seed=42,\n",
        "            executions_per_trial=2\n",
        "        )\n",
        "tuner_bo.search(X_train, to_categorical(y_train), epochs=100, validation_split=0.3, verbose=0,class_weight=class_weights)\n",
        "best_model = tuner_bo.get_best_models(num_models=1)[0]\n",
        "best_model.evaluate(X_test, to_categorical(y_test))\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fasjUKRpIez8"
      },
      "source": [
        "###Saving Trial Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUYH43o7d5Ur"
      },
      "source": [
        "!zip -r /content/Multi_keras.tuner.BayesOpt.zip /content/untitled_project"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR3b0lfbqhOV"
      },
      "source": [
        "#Analysing Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOAh847ct2Vb"
      },
      "source": [
        "##Input Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0_GJyDNtwT0"
      },
      "source": [
        "model = #input the model you want to analyse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP95hnrZqmVk"
      },
      "source": [
        "##Percentile f1 score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v51shqAlqgNb"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "res = model.predict_proba(X_train)\n",
        "th1=1-(y_train.target==0).sum()/y_train.target.count()*1\n",
        "th2=1-(y_train.target==1).sum()/y_train.target.count()*1\n",
        "c1=pd.DataFrame(res[:,0])\n",
        "c2=pd.DataFrame(res[:,1])\n",
        "print(f1_score(np.where(c1>c1.quantile(th1),1,0),np.where(y_train==0,1,0)))\n",
        "print(f1_score(np.where(c2>c2.quantile(th2),1,0),np.where(y_train==1,1,0)))\n",
        "res = model.predict_proba(X_test)\n",
        "th1=1-(y_test.target==0).sum()/y_test.target.count()*1\n",
        "th2=1-(y_test.target==1).sum()/y_test.target.count()*1\n",
        "c1=pd.DataFrame(res[:,0])\n",
        "c2=pd.DataFrame(res[:,1])\n",
        "print(f1_score(np.where(c1>c1.quantile(th1),1,0),np.where(y_test==0,1,0)))\n",
        "print(f1_score(np.where(c2>c2.quantile(th2),1,0),np.where(y_test==1,1,0)))\n",
        "res = model.predict_proba(timetest)\n",
        "th1=1-(y_timetest.target==0).sum()/y_timetest.target.count()*1\n",
        "th2=1-(y_timetest.target==1).sum()/y_timetest.target.count()*1\n",
        "c1=pd.DataFrame(res[:,0])\n",
        "c2=pd.DataFrame(res[:,1])\n",
        "print(f1_score(np.where(c1>c1.quantile(th1),1,0),np.where(y_timetest==0,1,0)))\n",
        "print(f1_score(np.where(c2>c2.quantile(th2),1,0),np.where(y_timetest==1,1,0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2GTIG6Iqxf0"
      },
      "source": [
        "##Gini per Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrezJ1liqsfS"
      },
      "source": [
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "gini =0\n",
        "#No of classes is j\n",
        "j=2\n",
        "y_predtrain=model.predict_proba(X_train)\n",
        "y_pred=model.predict_proba(X_test)\n",
        "y_predtime=model.predict_proba(timetest)\n",
        "drop_enc = OneHotEncoder().fit(pd.DataFrame(y_test))\n",
        "y_gini = drop_enc.transform(pd.DataFrame(y_test)).toarray()\n",
        "drop_enc = OneHotEncoder().fit(pd.DataFrame(y_timetest))\n",
        "y_ginitime = drop_enc.transform(pd.DataFrame(y_timetest)).toarray()\n",
        "drop_enc = OneHotEncoder().fit(pd.DataFrame(y_train))\n",
        "y_ginitrain = drop_enc.transform(pd.DataFrame(y_train)).toarray()\n",
        "for i in range(j):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_ginitrain[:, i], y_predtrain[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    print(2*roc_auc[i]-1)\n",
        "for i in range(j):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_gini[:, i], y_pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    print(2*roc_auc[i]-1)\n",
        "for i in range(j):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_ginitime[:, i], y_predtime[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    print(2*roc_auc[i]-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9_7rgRMq8y-"
      },
      "source": [
        "##Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI0_DuBAq7pC"
      },
      "source": [
        "print(accuracy_score(y_train,model.predict(X_train)))\n",
        "print(accuracy_score(y_test,model.predict(X_test)))\n",
        "print(accuracy_score(y_timetest,model.predict(timetest)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcSh-DIgtVA-"
      },
      "source": [
        "##Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkFkQDE2tS5S"
      },
      "source": [
        "y_pred = model.predict_classes(X_test)\n",
        "y_predtrain = model.predict_classes(X_train)\n",
        "y_predtime = model.predict_classes(timetest)\n",
        "print(confusion_matrix(y_train,y_predtrain))\n",
        "print(classification_report(y_train,y_predtrain))\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(confusion_matrix(y_timetest,y_predtime))\n",
        "print(classification_report(y_timetest,y_predtime))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yvKrgV6s-7p"
      },
      "source": [
        "#Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpuN5OoWt9Uy"
      },
      "source": [
        "##Input Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLbwKYg4t9VE"
      },
      "source": [
        "model_f = #input the model you want to find feature Importance\n",
        "model_name = #input model name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmQay1EIupMC"
      },
      "source": [
        "##Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrXl_VDVr0jF"
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "perm = permutation_importance(model_f,X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgaS3YoZCuxy"
      },
      "source": [
        "r = perm\n",
        "feature ={}\n",
        "for i in r.importances_mean.argsort()[::-1]:\n",
        "  if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
        "    feature[train.columns[i]]={'mean':r.importances_mean[i],'std':r.importances_std[i]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss_xrLsRC0jY"
      },
      "source": [
        "pd.DataFrame.from_dict(feature).T.to_excel('Multi_feature_neural.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}